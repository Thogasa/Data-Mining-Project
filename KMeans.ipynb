{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2f836e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.decomposition import PCA\n",
    "from collections import Counter \n",
    "\n",
    "#loading the dataset\n",
    "\n",
    "df1 = pd.read_csv('train.csv').drop(columns=['Artist Name', 'Track Name'])\n",
    "df2 = pd.read_csv('test.csv').drop(columns=['Artist Name', 'Track Name'])\n",
    "\n",
    "df = pd.concat([df1,df2])\n",
    "\n",
    "\n",
    "keys = df.keys()\n",
    "numcols = len(keys)\n",
    "df\n",
    "\n",
    "#check for missing values\n",
    "for k in keys:\n",
    "    print(f'column {k} has {np.round(df[k].isna().sum()/len(df)*100, 2)}% missing values')\n",
    "    \n",
    "#Populatirity has 2.55% missing values -> can be filled\n",
    "#key, instrumentalness and the Target have very high missing value rates -> mvs should be removed as to not alter the \n",
    "#distribution\n",
    "\n",
    "df = df[df['instrumentalness'].notna()]\n",
    "df = df[df['key'].notna()]\n",
    "df = df[df['Class'].notna()]\n",
    "\n",
    "target = df['Class']\n",
    "numTargets = len(set(target))\n",
    "df.drop(columns=['Class'], inplace = True)\n",
    "keys = df.keys()\n",
    "numcols = len(keys)\n",
    "\n",
    "target\n",
    "\n",
    "#Fill Popularity mv with median, because all popularities seem to be integer\n",
    "df.fillna(np.mean(df['Popularity']), inplace = True)\n",
    "df\n",
    "\n",
    "#Univariate Analysis\n",
    "means = {}\n",
    "medians = {}\n",
    "stdevs = {}\n",
    "for k in keys:\n",
    "    mean = np.mean(df[df[k].notna()][k])\n",
    "    med = np.median(df[df[k].notna()][k])\n",
    "    std = np.sqrt(np.mean((df[k]-mean)**2))\n",
    "    means[k] = mean\n",
    "    medians[k] = med\n",
    "    stdevs[k] = std\n",
    "\n",
    "means,medians,stdevs\n",
    "\n",
    "\n",
    "#Visualise each distribution using a boxplot\n",
    "n = 3\n",
    "m = 5\n",
    "fig, axs = plt.subplots(n,m)\n",
    "\n",
    "\n",
    "for i in range(n):\n",
    "    for j in range(m):\n",
    "        idx = i*m+j \n",
    "        if (idx < numcols):\n",
    "            axs[i][j].boxplot(df[keys[idx]])\n",
    "            axs[i][j].title.set_text(keys[idx])\n",
    "            \n",
    "plt.tight_layout()\n",
    "\n",
    "\n",
    "#Bivariate Analysis\n",
    "#Visualize each pair of variables\n",
    "\n",
    "axes = pd.plotting.scatter_matrix(df, alpha = 0.1, figsize = (15,15), c  =target);\n",
    "for ax in axes.flatten():\n",
    "    ax.xaxis.label.set_rotation(90)\n",
    "    ax.yaxis.label.set_rotation(0)\n",
    "    ax.yaxis.label.set_ha('right')\n",
    "    \n",
    "    \n",
    "#Normalisation\n",
    "#No strong non linear dependence can be seen in above plots.\n",
    "#as such a simple min max scaler to the [0,1] intervall is used\n",
    "maxs = np.max(df, axis = 0)\n",
    "mins = np.min(df, axis = 0)\n",
    "\n",
    "\n",
    "for k in keys:\n",
    "    df[k] = (df[k]-mins[k])/(maxs[k]-mins[k])\n",
    "\n",
    "#repeat means and stdev calculation\n",
    "means = {}\n",
    "medians = {}\n",
    "stdevs = {}\n",
    "for k in keys:\n",
    "    mean = np.mean(df[df[k].notna()][k])\n",
    "    med = np.median(df[df[k].notna()][k])\n",
    "    std = np.sqrt(np.mean((df[k]-mean)**2))\n",
    "    means[k] = mean\n",
    "    medians[k] = med\n",
    "    stdevs[k] = std\n",
    "\n",
    "    \n",
    "df\n",
    "\n",
    "\n",
    "#Calculate and visualize Correlation between each pair of variables\n",
    "\n",
    "correlations = df.corr()\n",
    "\n",
    "plt.matshow(correlations)\n",
    "plt.colorbar()\n",
    "plt.xticks(range(numcols), keys, rotation = 90);\n",
    "plt.yticks(range(numcols), keys, rotation = 0);\n",
    "correlations\n",
    "\n",
    "\n",
    "#Only one highly correlated variable pair obvious, which is \"energy\" and \"loudness\", a correlation that could have\n",
    "#been expected.\n",
    "#Witch a correlation of .795 this variable can be removed while barely losing any information.\n",
    "\n",
    "df.drop(columns=['loudness'], inplace = True)\n",
    "keys = df.keys()\n",
    "df\n",
    "\n",
    "\n",
    "#Map Each cluster label to what true label it most labels\n",
    "def mapPredAndTrue(tar, pred):\n",
    "    convMat = np.array(confusion_matrix(tar, pred, normalize = 'true'))\n",
    "    \n",
    "    \n",
    "    translation = np.argmax(convMat, axis = 0)\n",
    "\n",
    "    predLabel = pred.copy()\n",
    "\n",
    "    return translation[predLabel]\n",
    "\n",
    "\n",
    "from sklearn.cluster import AgglomerativeClustering\n",
    "\n",
    "from sklearn.metrics import confusion_matrix \n",
    "from sklearn.metrics import silhouette_score\n",
    "from sklearn.metrics import homogeneity_score\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "uppLimFak = 3\n",
    "silhouetteScores = []\n",
    "homogeneityScores = []\n",
    "\n",
    "clusterings = []\n",
    "possMetrics = ['euclidean', 'l1', 'l2', 'manhattan', 'cosine']\n",
    "nClustersToCheck = range(numTargets, uppLimFak*numTargets)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e9631d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import silhouette_score, homogeneity_score\n",
    "\n",
    "scaler = MinMaxScaler()        # normalization\n",
    "\n",
    "df_scaled = scaler.fit_transform(df)\n",
    "df_scaled = pd.DataFrame(df_scaled, columns=df.columns)\n",
    "\n",
    "\n",
    "k_values = range(3, uppLimFak * numTargets)     # range\n",
    "\n",
    "sc_silhouette = []\n",
    "sc_homogeneity = []\n",
    "\n",
    "for k in tqdm(k_values):  \n",
    "    \n",
    "    kmeans = KMeans(n_clusters=k, random_state=42)\n",
    "    kmeans.fit(df_scaled)\n",
    "    \n",
    "    sc_silhouette.append(silhouette_score(df_scaled, kmeans.labels_))\n",
    "    sc_homogeneity.append(homogeneity_score(mapPredAndTrue(target, kmeans.labels_), target))\n",
    "\n",
    "\n",
    "plt.plot(k_values, sc_silhouette, label='Silhouette Score')        # plotting homogenity and silhouette scores\n",
    "plt.plot(k_values, sc_homogeneity, label='Homogeneity Score')\n",
    "plt.xlabel('Number of Clusters')\n",
    "plt.ylabel('Score')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "\n",
    "best_k = k_values[np.argmax(sc_homogeneity)]     # best number of clusters according to homogeneity score\n",
    "\n",
    "\n",
    "best_kmeans = KMeans(n_clusters=best_k, random_state=42)    # applying KMeans\n",
    "best_kmeans.fit(df_scaled)\n",
    "\n",
    "\n",
    "axes = pd.plotting.scatter_matrix(df_scaled, alpha=0.1, figsize=(15, 15), c=best_kmeans.labels_)   # cluster visualization\n",
    "for ax in axes.flatten():\n",
    "    ax.xaxis.label.set_rotation(90)\n",
    "    ax.yaxis.label.set_rotation(0)\n",
    "    ax.yaxis.label.set_ha('right')\n",
    "plt.show()\n",
    "\n",
    "\n",
    "print(\"KMeans Silhouette Score:\", silhouette_score(df_scaled, best_kmeans.labels_))\n",
    "print(\"KMeans Homogeneity Score:\", homogeneity_score(mapPredAndTrue(target, best_kmeans.labels_), target))\n",
    "\n",
    "\n",
    "labels = best_kmeans.labels_                          # distances from points to their cluster's centroid\n",
    "distances = best_kmeans.transform(df_scaled)\n",
    "\n",
    "\n",
    "mean_dist = np.mean(distances, axis=1)     # mean distance for clusters\n",
    "\n",
    "\n",
    "outlier_th = 2 * np.mean(mean_dist)    # threshold for outliers\n",
    "\n",
    "\n",
    "\n",
    "outlier_ind = np.where(distances.max(axis=1) > outlier_th)[0]    # outlier indices identification\n",
    "\n",
    "\n",
    "df_dropped = df_scaled.drop(index=outlier_ind)   # removing outliers\n",
    "\n",
    "\n",
    "bestkm_dropped = KMeans(n_clusters=best_k, random_state=42)     # applying KMeans without outliers\n",
    "bestkm_dropped.fit(df_dropped)\n",
    "\n",
    "\n",
    "axes = pd.plotting.scatter_matrix(df_dropped, alpha=0.1, figsize=(15, 15), c=bestkm_dropped.labels_)    # cluster visualisation without outliers\n",
    "for ax in axes.flatten():\n",
    "    ax.xaxis.label.set_rotation(90)\n",
    "    ax.yaxis.label.set_rotation(0)\n",
    "    ax.yaxis.label.set_ha('right')\n",
    "plt.show()\n",
    "\n",
    "\n",
    "silhouette_dropped = silhouette_score(df_dropped, bestkm_dropped.labels_)\n",
    "homogeneity_dropped = homogeneity_score(mapPredAndTrue(target, bestkm_dropped.labels_), target)\n",
    "\n",
    "\n",
    "print(\"Silhouette Score (Otliers Removed):\", silhouette_dropped)    # scores after outliers removed, remain unchanged\n",
    "print(\"Homogeneity Score (Outliers Removed):\", homogeneity_dropped)\n",
    "\n",
    "\n",
    "kmeans_init = KMeans(n_clusters=best_k, init='k-means++', random_state=42)   #  applying KMeans with KMeans++ initialization\n",
    "kmeans_init.fit(df_scaled)\n",
    "\n",
    "\n",
    "axes = pd.plotting.scatter_matrix(df_scaled, alpha=0.1, figsize=(15, 15), c=kmeans_init.labels_)\n",
    "for ax in axes.flatten():\n",
    "    ax.xaxis.label.set_rotation(90)\n",
    "    ax.yaxis.label.set_rotation(0)\n",
    "    ax.yaxis.label.set_ha('right')\n",
    "plt.show()\n",
    "\n",
    "silhouette_kminit = silhouette_score(df_scaled, kmeans_init.labels_)        # scores with initialization, remains unchanged\n",
    "homogeneity_kminit = homogeneity_score(mapPredAndTrue(target, kmeans_init.labels_), target)\n",
    "\n",
    "print(\"KMeans++ Silhouette Score:\", silhouette_kminit)   \n",
    "print(\"KMeans++ Homogeneity Score:\", homogeneity_kminit)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
